{"cells":[{"cell_type":"markdown","source":["# An Introduction to Hail"],"metadata":{}},{"cell_type":"markdown","source":["Before we can get started with the genomic analysis, we need to first upgrade Databrick's version of Python to 3.6. We'll do so using Anaconda.\n\nFirst, download the Anaconda3 install script and place it in `/dbfs/tmp/` using the below command."],"metadata":{}},{"cell_type":"code","source":["%sh sudo wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh -O /dbfs/tmp/Anaconda3-5.2.0-Linux-x86_64.sh"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Next, run the following two cells to copy the Databrick's python packages to the Databrick's File System (DBFS). We'll use these later."],"metadata":{}},{"cell_type":"code","source":["%sh /databricks/python/bin/pip freeze > /tmp/python_packages.txt"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%fs cp file:/tmp/python_packages.txt dbfs:/tmp/python_packages.txt"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["In the below cell, set the `cluster-name` variable to the name of your cluster, then run the cell. Once complete, click on the cluster's name above (next to the green dot) and restart your cluster. This will install Anaconda, and all of Hail's dependencies. Note that in cluster environments, as long as Python versions between nodes match, it is only necessary to have the Anaconda libraries on the driver."],"metadata":{}},{"cell_type":"code","source":["cluster_name = \"mookus\"\nscript = \"\"\"#!/bin/bash\ncp /dbfs/tmp/Anaconda3-5.2.0-Linux-x86_64.sh /tmp\nsudo bash /tmp/Anaconda3-5.2.0-Linux-x86_64.sh -b -p /anaconda3\nmv /databricks/python /databricks/python_old\nln -s /anaconda3 /databricks/python\ncp /dbfs/tmp/python_packages.txt /tmp/python_packages.txt\n/databricks/python/bin/pip install --upgrade pip\n/databricks/python/bin/pip install -r --no-cache-dir /tmp/python_packages.txt\n/databricks/python/bin/conda update conda -y\n/databricks/python/bin/conda install -y numpy=1.14.0 pandas=0.22.0 matplotlib=2.2.3 seaborn=0.8.1 bokeh=0.12.13\n/databricks/python/bin/pip install parsimonious==0.8.0 ipykernel==4.9.0 decorator==4.3.0\n\"\"\"\ndbutils.fs.put(\"dbfs:/databricks/init/%s/install_conda.sh\" % cluster_name, script, True)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Once the cluster is restarted, run the below cell. If the output displays\n```\n3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n[GCC 7.2.0]```\nAnaconda was installed properly."],"metadata":{}},{"cell_type":"code","source":["%python\nimport sys\nprint (sys.version)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Getting Started\nBefore we can begin, let's import the Hail libraries and initialize Hail."],"metadata":{}},{"cell_type":"code","source":["import hail as hl\nimport hail.expr.aggregators as agg\nhl.init(sc)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Before using Hail, we import some standard Python libraries for use throughout the notebook."],"metadata":{}},{"cell_type":"code","source":["from pprint import pprint\nimport pandas as pd\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Span\noutput_notebook()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["###Check tutorial data for download if necessary\nThis cell downloads the necessary data if it isn’t already present."],"metadata":{}},{"cell_type":"code","source":["hl.utils.get_1kg('/tmp/data')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Load data from disk\nHail has its own internal data representation, called a MatrixTable. This is both an on-disk file format and a [Python object](https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable). Here, we read a MatrixTable from disk.\n\nThis dataset was created by downsampling a public 1000 genomes VCF to about 50 MB."],"metadata":{}},{"cell_type":"code","source":["mt = hl.read_matrix_table('/tmp/data/1kg.mt')"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Getting to know our data\nIt’s important to have easy ways to slice, dice, query, and summarize a dataset. Some of these methods are demonstrated below.\n\nThe [rows](https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.rows) method can be used to get a table with all the row fields in our MatrixTable.\n\nWe can use `rows` along with [select](https://hail.is/docs/0.2/hail.Table.html#hail.Table.select) to pull out 5 variants. The `select` method takes either a string refering to a field name in the table, or a Hail [Expression](https://hail.is/docs/0.2/expr/expression.html#hail.expr.expression.Expression). Here, we leave the arguments blank to keep only the row key fields, `locus` and `alleles`.\n\nUse the `show` method to display the variants."],"metadata":{}},{"cell_type":"code","source":["mt.rows().select().show(5)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Here is how to peek at the first few sample IDs:"],"metadata":{}},{"cell_type":"code","source":["mt.s.show(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["To look at the first few genotype calls, we can use [entries](https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.entries) along with `select` and `take`. The `take` method collects the first n rows into a list. Alternatively, we can use the `show` method, which prints the first n rows to the console in a table format.\n\nTry changing `take` to `show` in the cell below."],"metadata":{}},{"cell_type":"code","source":["mt.entry.take(5)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Adding Column Fields\nA Hail MatrixTable can have any number of row fields and column fields for storing data associated with each row and column. Annotations are usually a critical part of any genetic study. Column fields are where you’ll store information about sample phenotypes, ancestry, sex, and covariates. Row fields can be used to store information like gene membership and functional impact for use in QC or analysis.\n\nIn this tutorial, we demonstrate how to take a text file and use it to annotate the columns in a MatrixTable.\n\nThe file provided contains the sample ID, the population and “super-population” designations, the sample sex, and two simulated phenotypes (one binary, one discrete).\n\nThis file can be imported into Hail with [import_table](https://hail.is/docs/0.2/methods/impex.html#hail.methods.import_table). This method produces a [Table](https://hail.is/docs/0.2/hail.Table.html#hail.Table) object. Think of this as a Pandas or R dataframe that isn’t limited by the memory on your machine – behind the scenes, it’s distributed with Spark."],"metadata":{}},{"cell_type":"code","source":["table = (hl.import_table('/tmp/data/1kg_annotations.txt', impute=True)\n         .key_by('Sample'))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["A good way to peek at the structure of a `Table` is to look at its `schema`."],"metadata":{}},{"cell_type":"code","source":["table.describe()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["To peek at the first few values, use the `show` method:"],"metadata":{}},{"cell_type":"code","source":["table.show(width=100)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Now we’ll use this table to add sample annotations to our dataset, storing the annotations in column fields in our MatrixTable. First, we’ll print the existing column schema:"],"metadata":{}},{"cell_type":"code","source":["print(mt.col.dtype)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["We use the [annotate_cols](https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.annotate_cols) method to join the table with the MatrixTable containing our dataset."],"metadata":{}},{"cell_type":"code","source":["mt = mt.annotate_cols(**table[mt.s])"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["print(mt.col.dtype)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["print(mt.col.dtype.pretty())"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["mt.rows().describe()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["mt.cols().describe()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["###Query functions and the Hail Expression Language\nHail has a number of useful query functions that can be used for gathering statistics on our dataset. These query functions take Hail Expressions as arguments.\n\nWe will start by looking at some statistics of the information in our table. The [aggregate](https://hail.is/docs/0.2/hail.Table.html#hail.Table.aggregate) method can be used to aggregate over rows of the table.\n\n`counter` is an aggregation function that counts the number of occurrences of each unique element. We can use this to pull out the population distribution by passing in a Hail Expression for the field that we want to count by."],"metadata":{}},{"cell_type":"code","source":["pprint(table.aggregate(agg.counter(table.SuperPopulation)))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["`stats` is an aggregation function that produces some useful statistics about numeric collections. We can use this to see the distribution of the CaffeineConsumption phenotype."],"metadata":{}},{"cell_type":"code","source":["pprint(table.aggregate(agg.stats(table.CaffeineConsumption)))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["However, these metrics aren’t perfectly representative of the samples in our dataset. Here’s why:"],"metadata":{}},{"cell_type":"code","source":["table.count()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["mt.count_cols()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Since there are fewer samples in our dataset than in the full thousand genomes cohort, we need to look at annotations on the dataset. We can use [aggregate_cols](https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.aggregate_cols) to get the metrics for only the samples in our dataset."],"metadata":{}},{"cell_type":"code","source":["mt.aggregate_cols(agg.counter(mt.SuperPopulation))"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["pprint(mt.aggregate_cols(agg.stats(mt.CaffeineConsumption)))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["The functionality demonstrated in the last few cells isn’t anything especially new: it’s certainly not difficult to ask these questions with Pandas or R dataframes, or even Unix tools like `awk`. But Hail can use the same interfaces and query language to analyze collections that are much larger, like the set of variants.\n\nHere we calculate the counts of each of the 12 possible unique SNPs (4 choices for the reference base * 3 choices for the alternate base).\n\nTo do this, we need to get the alternate allele of each variant and then count the occurences of each unique ref/alt pair. This can be done with Hail’s `counter` method."],"metadata":{}},{"cell_type":"code","source":["snp_counts = mt.aggregate_rows(agg.counter(hl.Struct(ref=mt.alleles[0], alt=mt.alleles[1])))\npprint(snp_counts)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["We can list the counts in descending order using Python’s Counter class."],"metadata":{}},{"cell_type":"code","source":["from collections import Counter\ncounts = Counter(snp_counts)\ncounts.most_common()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["It’s nice to see that we can actually uncover something biological from this small dataset: we see that these frequencies come in pairs. C/T and G/A are actually the same mutation, just viewed from from opposite strands. Likewise, T/A and A/T are the same mutation on opposite strands. There’s a 30x difference between the frequency of C/T and A/T SNPs. Why?\n\nThe same Python, R, and Unix tools could do this work as well, but we’re starting to hit a wall - the latest [gnomAD release](http://gnomad.broadinstitute.org/) publishes about 250 million variants, and that won’t fit in memory on a single computer.\n\nWhat about genotypes? Hail can query the collection of all genotypes in the dataset, and this is getting large even for our tiny dataset. Our 284 samples and 10,000 variants produce 10 million unique genotypes. The gnomAD dataset has about **5 trillion** unique genotypes.\n\nHail plotting functions allow Hail fields as arguments, so we can pass in the DP field directly here. If the range and bins arguments are not set, this function will compute the range based on minimum and maximum values of the field and use the default 50 bins."],"metadata":{}},{"cell_type":"code","source":["from bokeh.plotting import figure\nfrom bokeh.embed import components, file_html\nfrom bokeh.resources import CDN\np = hl.plot.histogram(mt.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP')\nhtml = file_html(p, CDN, \"DP Histogram\")\ndisplayHTML(html)\n"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["### Quality Control\nQC is where analysts spend most of their time with sequencing datasets. QC is an iterative process, and is different for every project: there is no “push-button” solution for QC. Each time the Broad collects a new group of samples, it finds new batch effects. However, by practicing open science and discussing the QC process and decisions with others, we can establish a set of best practices as a community.\n\nQC is entirely based on the ability to understand the properties of a dataset. Hail attempts to make this easier by providing the [sample_qc](https://hail.is/docs/0.2/methods/genetics.html#hail.methods.sample_qc) method, which produces a set of useful metrics and stores them in a column field."],"metadata":{}},{"cell_type":"code","source":["mt.col.describe()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["mt = hl.sample_qc(mt)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["mt.col.describe()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["Plotting the QC metrics is a good place to start."],"metadata":{}},{"cell_type":"code","source":["p = hl.plot.histogram(mt.sample_qc.call_rate, range=(.88,1), legend='Call Rate')\nhtml = file_html(p, CDN, \"Call Rate\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["p = hl.plot.histogram(mt.sample_qc.gq_stats.mean, range=(10,70), legend='Mean Sample GQ')\nhtml = file_html(p, CDN, \"Mean Sample GQ\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Often, these metrics are correlated."],"metadata":{}},{"cell_type":"code","source":["p = hl.plot.scatter(mt.sample_qc.dp_stats.mean, mt.sample_qc.call_rate, xlabel='Mean DP', ylabel='Call Rate')\nhtml = file_html(p, CDN, \"Mean DP vs Call Rate\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["Removing outliers from the dataset will generally improve association results. We can draw lines on the above plot to indicate outlier cuts."],"metadata":{}},{"cell_type":"code","source":["p.renderers.extend(\n    [Span(location=0.97, dimension='width', line_color='black', line_width=1),\n     Span(location=4, dimension='height', line_color='black', line_width=1)])\nhtml = file_html(p, CDN, \"Mean DP vs Call Rate\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["We’ll want to remove all samples that fall in the bottom left quadrant.\n\nIt’s easy to filter when we’ve got the cutoff values decided:"],"metadata":{}},{"cell_type":"code","source":["mt = mt.filter_cols((mt.sample_qc.dp_stats.mean >= 4) & (mt.sample_qc.call_rate >= 0.97))\nprint('After filter, %d/284 samples remain.' % mt.count_cols())"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["Next is genotype QC. To start, we’ll print the post-sample-QC call rate. It’s actually gone up since the initial summary - dropping low-quality samples disproportionally removed missing genotypes.\n\nImport the `hail.expr.functions` module."],"metadata":{}},{"cell_type":"code","source":["call_rate = mt.aggregate_entries(agg.fraction(hl.is_defined(mt.GT)))\nprint('pre QC call rate is %.3f' % call_rate)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["It’s a good idea to filter out genotypes where the reads aren’t where they should be: if we find a genotype called homozygous reference with >10% alternate reads, a genotype called homozygous alternate with >10% reference reads, or a genotype called heterozygote without a ref / alt balance near 1:1, it is likely to be an error."],"metadata":{}},{"cell_type":"code","source":["ab = mt.AD[1] / hl.sum(mt.AD)\n\nfilter_condition_ab = ((mt.GT.is_hom_ref() & (ab <= 0.1)) |\n                        (mt.GT.is_het() & (ab >= 0.25) & (ab <= 0.75)) |\n                        (mt.GT.is_hom_var() & (ab >= 0.9)))\n\nmt = mt.filter_entries(filter_condition_ab)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["post_qc_call_rate = mt.aggregate_entries(agg.fraction(hl.is_defined(mt.GT)))\nprint('post QC call rate is %.3f' % post_qc_call_rate)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["Variant QC is a bit more of the same: we can use the [variant_qc](https://hail.is/docs/0.2/methods/genetics.html?highlight=variant%20qc#hail.methods.variant_qc) method to produce a variety of useful statistics, plot them, and filter."],"metadata":{}},{"cell_type":"code","source":["mt.row.describe()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["mt = hl.variant_qc(mt).cache()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["mt.row.describe()"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["qc_table = mt.rows()\nqc_table = qc_table.select(gq_mean = qc_table.variant_qc.gq_stats.mean,\n                           call_rate = qc_table.variant_qc.call_rate,\n                           p_value_hwe = qc_table.variant_qc.p_value_hwe,\n                           AAF = qc_table.variant_qc.AF[1],)\n\np1 = hl.plot.histogram(qc_table.gq_mean, range=(10,80), legend='Variant Mean GQ')\n\np2 = hl.plot.histogram(qc_table.AAF, range=(0,1), legend='Alternate Allele Frequency')\n\np3 = hl.plot.histogram(qc_table.call_rate, range=(.5,1), legend='Variant Call Rate')\n\np4 = hl.plot.histogram(qc_table.p_value_hwe, range=(0,1), legend='Hardy-Weinberg Equilibrium p-value')\n\ng = gridplot([[p1, p2], [p3, p4]])\n\nhtml = file_html(g, CDN, \"Mean DP vs Call Rate\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["These statistics actually look pretty good: we don’t need to filter this dataset. Most datasets require thoughtful quality control, though. The [filter_rows])(https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.filter_rows) method can help!"],"metadata":{}},{"cell_type":"markdown","source":["### Let's do a GWAS\nFirst, we need to restrict to variants that are:\n\n* common (we’ll use a cutoff of 1%)\n* uncorrelated (not in linkage disequilibrium)"],"metadata":{}},{"cell_type":"code","source":["common_mt = mt.filter_rows(mt.variant_qc.AF[1] > 0.01)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["print('Samples: %d  Variants: %d' % (common_mt.count_cols(), common_mt.count_rows()))"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["These filters removed about 15% of sites (we started with a bit over 10,000). This is NOT representative of most sequencing datasets! We have already downsampled the full thousand genomes dataset to include more common variants than we’d expect by chance.\n\nIn Hail, the association tests accept column fields for the sample phenotype and covariates. Since we’ve already got our phenotype of interest (caffeine consumption) in the dataset, we are good to go:"],"metadata":{}},{"cell_type":"code","source":["gwas = hl.linear_regression_rows(y=common_mt.CaffeineConsumption, x=common_mt.GT.n_alt_alleles(), covariates=[1.0])\ngwas.row.describe()"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["Looking at the bottom of the above printout, you can see the linear regression adds new row fields for the beta, standard error, t-statistic, and p-value.\n\nHail makes it easy to make a [Q-Q (quantile-quantile) plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot)."],"metadata":{}},{"cell_type":"code","source":["p = hl.plot.qq(gwas.p_value)\n\nhtml = file_html(p, CDN, \"GWAS p-value\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["###Confounded!\nThe observed p-values drift away from the expectation immediately. Either every SNP in our dataset is causally linked to caffeine consumption (unlikely), or there’s a confounder.\n\nWe didn’t tell you, but sample ancestry was actually used to simulate this phenotype. This leads to a [stratified](https://en.wikipedia.org/wiki/Population_stratification) distribution of the phenotype. The solution is to include ancestry as a covariate in our regression.\n\nThe [linear_regression](https://hail.is/docs/0.2/methods/stats.html#hail.methods.linear_regression) method can also take column fields to use as covariates. We already annotated our samples with reported ancestry, but it is good to be skeptical of these labels due to human error. Genomes don’t have that problem! Instead of using reported ancestry, we will use genetic ancestry by including computed principal components in our model.\n\nThe [pca](https://hail.is/docs/0.2/methods/stats.html#hail.methods.pca) method produces eigenvalues as a list and sample PCs as a Table, and can also produce variant loadings when asked. The [hwe_normalized_pca](https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca) method does the same, using HWE-normalized genotypes for the PCA."],"metadata":{}},{"cell_type":"code","source":["pca_eigenvalues, pca_scores, _ = hl.hwe_normalized_pca(common_mt.GT)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["pprint(pca_eigenvalues)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["p = hl.plot.scatter(pca_scores.scores[0], pca_scores.scores[1],\n                    label=common_mt.cols()[pca_scores.s].SuperPopulation,\n                    title='PCA', xlabel='PC1', ylabel='PC2')\nhtml = file_html(p, CDN, \"PCA\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["Now we can rerun our linear regression, controlling for sample sex and the first few principal components. We’ll do this with input variable the number of alternate alleles as before, and again with input variable the genotype dosage derived from the PL field."],"metadata":{}},{"cell_type":"code","source":["cmt = common_mt.annotate_cols(pca = pca_scores[common_mt.s])\n\nlinear_regression_results = hl.linear_regression_rows(\n    y=cmt.CaffeineConsumption, x=cmt.GT.n_alt_alleles(),\n    covariates=[1.0, cmt.isFemale, cmt.pca.scores[0], cmt.pca.scores[1], cmt.pca.scores[2]])\n  \np = hl.plot.qq(linear_regression_results.p_value)\nhtml = file_html(p, CDN, \"Q-Q Plot\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"markdown","source":["We can also use the genotype dosage (the probability-weighted expected number of alternate alleles) instead of the hard call in our regression"],"metadata":{}},{"cell_type":"code","source":["def plDosage(pl):\n    linear_scaled = pl.map(lambda x: 10 ** (-x / 10))\n    pl_sum = hl.sum(linear_scaled)\n    normed = linear_scaled / pl_sum\n    return 1 * normed[1] + 2 * normed[2]"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["linear_regression_results = hl.linear_regression_rows(\n    y=cmt.CaffeineConsumption, x=plDosage(cmt.PL),\n    covariates=[1.0, cmt.isFemale, cmt.pca.scores[0], cmt.pca.scores[1], cmt.pca.scores[2]])"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["p = hl.plot.qq(linear_regression_results.p_value)\nhtml = file_html(p, CDN, \"Q-Q Plot\")\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["That’s more like it! We may not be publishing ten new coffee-drinking loci in Nature, but we shouldn’t expect to find anything but the strongest signals from a dataset of 284 individuals anyway."],"metadata":{}},{"cell_type":"markdown","source":["###Rare variant analysis\nHere we’ll demonstrate how one can use the expression language to group and count by any arbitrary properties in row and column fields. Hail also implements the sequence kernel association test (SKAT)."],"metadata":{}},{"cell_type":"code","source":["entries = mt.entries()\nresults = (entries.group_by(pop = entries.SuperPopulation, chromosome = entries.locus.contig)\n      .aggregate(n_het = agg.count_where(entries.GT.is_het())))"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["results.show()"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"markdown","source":["What if we want to group by minor allele frequency bin and hair color, and calculate the mean GQ?"],"metadata":{}},{"cell_type":"code","source":["entries = entries.annotate(maf_bin = hl.cond(entries.info.AF[0]<0.01, \"< 1%\",\n                             hl.cond(entries.info.AF[0]<0.05, \"1%-5%\", \">5%\")))\n\nresults2 = (entries.group_by(af_bin = entries.maf_bin, purple_hair = entries.PurpleHair)\n      .aggregate(mean_gq = agg.stats(entries.GQ).mean,\n                 mean_dp = agg.stats(entries.DP).mean))"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["results2.show()"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"markdown","source":["We’ve shown that it’s easy to aggregate by a couple of arbitrary statistics. This specific examples may not provide especially useful pieces of information, but this same pattern can be used to detect effects of rare variation:\n\n* Count the number of heterozygous genotypes per gene by functional category (synonymous, missense, or loss-of-function) to estimate per-gene functional constraint* \n* Count the number of singleton loss-of-function mutations per gene in cases and controls to detect genes involved in disease"],"metadata":{}},{"cell_type":"markdown","source":["###Epilogue\nCongrats! You’ve reached the end of the first tutorial. To learn more about Hail’s API and functionality, take a look at the other tutorials. You can check out the [Python API](https://hail.is/docs/0.2/api.html#python-api) for documentation on additional Hail functions. If you use Hail for your own science, we’d love to hear from you on [Zulip chat](https://hail.zulipchat.com/) or the [discussion forum](http://discuss.hail.is/)."],"metadata":{}},{"cell_type":"markdown","source":["### Population Clustering Using K-Means in SparkML\nIn this section, we'll investigate how to leverage Spark's native machine learning libraries to glean insights from our genomic data. In particular, K-Means clustering will be applied to each sample's principal components and the resulting classifications will be compared to their initial Super Population labeling. \n\nHail's API supports exporting objects to the standard Spark ecosystem. The below snippet places our PCA data, sample data, and super population data in a DataFrame."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.ml.linalg import Vectors, VectorUDT\narray_to_vec_udf = udf(lambda a: Vectors.dense(a), VectorUDT())\n\ndf = cmt.cols().to_spark().select(col('s'), col('Population'), col('SuperPopulation'), array_to_vec_udf(col('`pca.scores`')).alias('pca'))\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":["Next, we'll leverage Spark's ML documentation to build a Machine Learning pipeline that performs K-Means clustering on the PCA vectors. (Note that as this could be accomplished without a pipeline as it is a single stage.) \n\nPlease reference the below documentation on pipelines to see how to assemble the ML pipeline.\nhttps://spark.apache.org/docs/latest/ml-pipeline.html"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import KMeans\n\n\nkmeans = KMeans(k=5, featuresCol=\"pca\", predictionCol='prediction')\npipeline = Pipeline(stages=[kmeans])"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"markdown","source":["Next, we'll fit our DataFrame to the pipline and subsequently generate the K-Means predictions. The rest of the snippet simply converts the resulting Spark DataFrame into a Pandas DataFrame for later visualization."],"metadata":{}},{"cell_type":"code","source":["model = pipeline.fit(df) # Fill in with the code to fit the DataFrame to the\n                         # pipeline.\nresult_df = model.transform(df) # Fill in with the code to \n                                # generate the predictions.\npdf = result_df.select('s', 'SuperPopulation', 'prediction').toPandas()\nbase_pdf = pdf\npdf['index'] = pdf.index\npdf.SuperPopulation = pd.Categorical(pdf.SuperPopulation)\npdf['SuperPopulationCodes'] = pdf.SuperPopulation.cat.codes\npdf.head()"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":["Finally, run the below cells to generate a stacked bar plot to examine a comparison of how K-means clustered the samples compared to their `SuperPopulations`."],"metadata":{}},{"cell_type":"code","source":["eur0 = pdf[(pdf.SuperPopulation=='EUR') & (pdf.prediction==0)].shape[0]\neur1 = pdf[(pdf.SuperPopulation=='EUR') & (pdf.prediction==1)].shape[0]\neur2 = pdf[(pdf.SuperPopulation=='EUR') & (pdf.prediction==2)].shape[0]\neur3 = pdf[(pdf.SuperPopulation=='EUR') & (pdf.prediction==3)].shape[0]\neur4 = pdf[(pdf.SuperPopulation=='EUR') & (pdf.prediction==4)].shape[0]\neurs = (eur0, eur1, eur2, eur3, eur4)\n\namr0 = pdf[(pdf.SuperPopulation=='AMR') & (pdf.prediction==0)].shape[0]\namr1 = pdf[(pdf.SuperPopulation=='AMR') & (pdf.prediction==1)].shape[0]\namr2 = pdf[(pdf.SuperPopulation=='AMR') & (pdf.prediction==2)].shape[0]\namr3 = pdf[(pdf.SuperPopulation=='AMR') & (pdf.prediction==3)].shape[0]\namr4 = pdf[(pdf.SuperPopulation=='AMR') & (pdf.prediction==4)].shape[0]\namrs = (amr0, amr1, amr2, amr3, amr4)\n\neas0 = pdf[(pdf.SuperPopulation=='EAS') & (pdf.prediction==0)].shape[0]\neas1 = pdf[(pdf.SuperPopulation=='EAS') & (pdf.prediction==1)].shape[0]\neas2 = pdf[(pdf.SuperPopulation=='EAS') & (pdf.prediction==2)].shape[0]\neas3 = pdf[(pdf.SuperPopulation=='EAS') & (pdf.prediction==3)].shape[0]\neas4 = pdf[(pdf.SuperPopulation=='EAS') & (pdf.prediction==4)].shape[0]\neass = (eas0, eas1, eas2, eas3, eas4)\n\nafr0 = pdf[(pdf.SuperPopulation=='AFR') & (pdf.prediction==0)].shape[0]\nafr1 = pdf[(pdf.SuperPopulation=='AFR') & (pdf.prediction==1)].shape[0]\nafr2 = pdf[(pdf.SuperPopulation=='AFR') & (pdf.prediction==2)].shape[0]\nafr3 = pdf[(pdf.SuperPopulation=='AFR') & (pdf.prediction==3)].shape[0]\nafr4 = pdf[(pdf.SuperPopulation=='AFR') & (pdf.prediction==4)].shape[0]\nafrs = (afr0, afr1, afr2, afr3, afr4)\n\nsas0 = pdf[(pdf.SuperPopulation=='SAS') & (pdf.prediction==0)].shape[0]\nsas1 = pdf[(pdf.SuperPopulation=='SAS') & (pdf.prediction==1)].shape[0]\nsas2 = pdf[(pdf.SuperPopulation=='SAS') & (pdf.prediction==2)].shape[0]\nsas3 = pdf[(pdf.SuperPopulation=='SAS') & (pdf.prediction==3)].shape[0]\nsas4 = pdf[(pdf.SuperPopulation=='SAS') & (pdf.prediction==4)].shape[0]\nsass = (sas0, sas1, sas2, sas3, sas4)"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["import operator\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.close(\"all\")\n\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 4, 5)\nwidth = 0.35\n\nax.bar(x, eurs, width)\nbot=eurs\nax.bar(x, amrs, width, bottom=bot)\nbot = tuple(map(operator.add, bot, amrs))\nax.bar(x, eass, width, bottom=bot)\nbot = tuple(map(operator.add, bot, eass))\nax.bar(x, afrs, width,bottom=bot)\nbot = tuple(map(operator.add, bot, afrs))\nax.bar(x, sass, width, bottom=bot)\nax.set_ylabel('Superpop Counts per Prediction')\nax.set_title('K-Means Clusters vs. SuperPopulations')\nax.set_xticks(x, ('0', '1', '2', '3', '4'))\nax.legend(('EUR', 'AMR', 'EAS', 'AFR', 'SAS'))\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":113}],"metadata":{"name":"hail-tutorial-spark-saturday-advanced","notebookId":2375953204134729},"nbformat":4,"nbformat_minor":0}
